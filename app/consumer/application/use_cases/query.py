"""Consumer Query Use Cases

Consumer Group ì¡°íšŒ ê´€ë ¨ Use Caseë“¤ì„ í†µí•©

"""

import logging
from collections.abc import Awaitable, Callable
from contextlib import AbstractAsyncContextManager
from datetime import timedelta

from confluent_kafka.admin import AdminClient
from sqlalchemy import desc, func, select
from sqlalchemy.ext.asyncio import AsyncSession

from app.consumer.domain.models import ConsumerPartition, RebalanceRollup
from app.consumer.domain.services import (
    ConsumerDataCollector,
    ConsumerMetricsCalculator,
    StuckPartitionDetector,
)
from app.consumer.domain.thresholds import DEFAULT_THRESHOLDS
from app.consumer.domain.types_enum import WindowType
from app.consumer.infrastructure.kafka_consumer_adapter import KafkaConsumerAdapter
from app.consumer.infrastructure.models import (
    ConsumerGroupRebalanceDeltaModel,
    ConsumerPartitionSnapshotModel,
)
from app.consumer.infrastructure.repository import ConsumerRepository
from app.consumer.interface.schema import (
    ConsumerGroupListResponse,
    ConsumerGroupResponse,
    LagStatsResponse,
)
from app.consumer.interface.schema.detail_schema import (
    ConsumerGroupSummaryResponse,
    MemberDetailResponse,
    PartitionDetailResponse,
    RebalanceEventResponse,
    TopicConsumerMappingResponse,
)


class ListConsumerGroupsUseCase:
    """Consumer Group ëª©ë¡ ì¡°íšŒ Use Case - ì‹¤ì‹œê°„ Kafka ì¡°íšŒ"""

    def __init__(self, admin_client_getter: Callable[[str], Awaitable[AdminClient]]) -> None:
        """Use case ìƒì„±ì

        Args:
            admin_client_getter: cluster_idë¡œ AdminClientë¥¼ ê°€ì ¸ì˜¤ëŠ” async í•¨ìˆ˜
        """
        self._admin_client_getter = admin_client_getter

    async def execute(self, cluster_id: str) -> ConsumerGroupListResponse:
        """Consumer Group ëª©ë¡ ì¡°íšŒ - ì‹¤ì‹œê°„ Kafka ì¡°íšŒ

        Args:
            cluster_id: í´ëŸ¬ìŠ¤í„° ID

        Returns:
            ConsumerGroupListResponse

        Raises:
            ValueError: AdminClientë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ
        """
        # 1. AdminClient ë° Adapter ìƒì„±
        admin_client = await self._admin_client_getter(cluster_id)

        adapter = KafkaConsumerAdapter(admin_client)
        collector = ConsumerDataCollector(adapter, cluster_id)

        # 2. ëª¨ë“  Consumer Group ëª©ë¡ ì¡°íšŒ
        try:
            group_infos = await adapter.list_consumer_groups()
            all_groups = [
                await collector.collect_group(group_info.group_id) for group_info in group_infos
            ]
        except Exception:
            # ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            return ConsumerGroupListResponse(groups=[], total=0)

        # 3. Response DTO ë³€í™˜
        groups = [
            ConsumerGroupResponse(
                cluster_id=group.cluster_id,
                group_id=group.group_id,
                ts=group.ts,
                state=group.state.value,
                partition_assignor=group.partition_assignor.value
                if group.partition_assignor
                else None,
                member_count=group.member_count,
                topic_count=group.topic_count,
                lag_stats=LagStatsResponse(
                    total_lag=group.lag_stats.total_lag,
                    mean_lag=group.lag_stats.mean_lag,
                    p50_lag=group.lag_stats.p50_lag,
                    p95_lag=group.lag_stats.p95_lag,
                    max_lag=group.lag_stats.max_lag,
                    partition_count=group.topic_count,
                ),
            )
            for group in all_groups
        ]

        return ConsumerGroupListResponse(
            groups=groups,
            total=len(groups),
        )


class GetConsumerGroupSummaryUseCase:
    """Consumer Group Summary Use Case - ì‹¤ì‹œê°„ Kafka ì¡°íšŒ ë°©ì‹"""

    def __init__(
        self,
        admin_client_getter: Callable[[str], Awaitable[AdminClient]],
        session_factory: Callable[[], AbstractAsyncContextManager[AsyncSession]] | None = None,
    ) -> None:
        """Use case ìƒì„±ì

        Args:
            admin_client_getter: cluster_idë¡œ AdminClientë¥¼ ê°€ì ¸ì˜¤ëŠ” async í•¨ìˆ˜
            session_factory: Delta ì¡°íšŒìš© ì„¸ì…˜ íŒ©í† ë¦¬ (optional)
        """
        self._admin_client_getter = admin_client_getter
        self._session_factory = session_factory
        self._calculator = ConsumerMetricsCalculator()

    async def execute(self, cluster_id: str, group_id: str) -> ConsumerGroupSummaryResponse:
        """ê·¸ë£¹ Summary ìƒì„± - ì‹¤ì‹œê°„ Kafka ì¡°íšŒ

        Args:
            cluster_id: í´ëŸ¬ìŠ¤í„° ID
            group_id: Consumer Group ID

        Raises:
            ValueError: AdminClientë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ê·¸ë£¹ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ
        """
        # 1. AdminClient ë° Adapter ìƒì„±
        admin_client = await self._admin_client_getter(cluster_id)

        adapter = KafkaConsumerAdapter(admin_client)
        collector = ConsumerDataCollector(adapter, cluster_id)

        # 2. ì‹¤ì‹œê°„ Kafka ì¡°íšŒ
        try:
            group = await collector.collect_group(group_id)
            members = await collector.collect_members(group_id)
            partitions = await collector.collect_partitions(group_id)
        except KeyError as e:
            raise ValueError(f"Consumer group not found: {group_id}") from e

        # 3. ê³µì •ì„±(Gini) ê³„ì‚° - ì‹¤ì‹œê°„ ë°ì´í„°ë¡œ
        fairness = self._calculator.calculate_fairness(members, partitions)

        # 4. ë¦¬ë°¸ëŸ°ìŠ¤ ì ìˆ˜ ë° Stuck íŒŒí‹°ì…˜ (Delta ì¡°íšŒ, optional)
        # ê¸°ë³¸ê°’: None (ë°ì´í„° ì—†ìœ¼ë©´ "ì•Œ ìˆ˜ ì—†ìŒ")
        rebalance_score: float | None = None
        stuck_partitions: list[dict[str, object]] = []
        if self._session_factory is not None:
            async with self._session_factory() as session:
                repo = ConsumerRepository(session)

                rollup_model = await repo.get_latest_rollup(
                    cluster_id, group_id, WindowType.ONE_HOUR
                )
                if rollup_model is not None:
                    rollup = RebalanceRollup(
                        cluster_id=rollup_model.cluster_id,
                        group_id=rollup_model.group_id,
                        window_start=rollup_model.window_start,
                        window=WindowType(rollup_model.window),
                        rebalances=rollup_model.rebalances,
                        avg_moved_partitions=rollup_model.avg_moved_partitions,
                        max_moved_partitions=rollup_model.max_moved_partitions,
                        stable_ratio=rollup_model.stable_ratio,
                    )
                    rebalance_score = rollup.rebalance_score()

                stuck_partitions = await self._get_stuck_partitions(
                    session, repo, cluster_id, group_id
                )

        # 5. Summary ì‘ë‹µ ì¡°ë¦½ - ì‹¤ì‹œê°„ ë°ì´í„° ì‚¬ìš©
        return ConsumerGroupSummaryResponse(
            group_id=group_id,
            cluster_id=cluster_id,
            state=group.state.value,
            member_count=group.member_count,
            topic_count=len({p.topic for p in partitions}),
            lag={
                "p50": group.lag_stats.p50_lag,
                "p95": group.lag_stats.p95_lag,
                "max": group.lag_stats.max_lag,
                "total": group.lag_stats.total_lag,
            },
            rebalance_score=rebalance_score,
            fairness_gini=fairness.gini_coefficient,
            stuck=stuck_partitions,
        )

    async def _get_stuck_partitions(
        self,
        session: AsyncSession,
        repo: ConsumerRepository,
        cluster_id: str,
        group_id: str,
    ) -> list[dict[str, object]]:
        """DB ìŠ¤ëƒ…ìƒ·ì„ í™œìš©í•œ Stuck Partition ê°ì§€"""
        latest_snapshot = await repo.get_latest_group_snapshot(cluster_id, group_id)
        if latest_snapshot is None:
            return []

        current_models = await repo.get_partition_snapshots(
            cluster_id=cluster_id, group_id=group_id, ts=latest_snapshot.ts
        )
        if not current_models:
            return []

        thresholds = DEFAULT_THRESHOLDS.stuck
        threshold_ts = latest_snapshot.ts - timedelta(seconds=thresholds.duration_s_ge)

        previous_subquery = (
            select(
                ConsumerPartitionSnapshotModel.topic.label("topic"),
                ConsumerPartitionSnapshotModel.partition.label("partition"),
                func.max(ConsumerPartitionSnapshotModel.ts).label("ts"),
            )
            .where(
                ConsumerPartitionSnapshotModel.cluster_id == cluster_id,
                ConsumerPartitionSnapshotModel.group_id == group_id,
                ConsumerPartitionSnapshotModel.ts <= threshold_ts,
            )
            .group_by(
                ConsumerPartitionSnapshotModel.topic,
                ConsumerPartitionSnapshotModel.partition,
            )
            .subquery()
        )

        previous_result = await session.execute(
            select(ConsumerPartitionSnapshotModel).join(
                previous_subquery,
                (
                    (ConsumerPartitionSnapshotModel.topic == previous_subquery.c.topic)
                    & (ConsumerPartitionSnapshotModel.partition == previous_subquery.c.partition)
                    & (ConsumerPartitionSnapshotModel.ts == previous_subquery.c.ts)
                ),
            )
        )
        previous_models = list(previous_result.scalars().all())
        if not previous_models:
            return []

        current_partitions = [self._snapshot_to_partition(model) for model in current_models]
        previous_partitions = [self._snapshot_to_partition(model) for model in previous_models]

        detector = StuckPartitionDetector(
            epsilon=thresholds.delta_committed_le,
            theta=thresholds.delta_lag_ge,
            min_duration_seconds=thresholds.duration_s_ge,
        )

        previous_map = {
            (p.cluster_id, p.group_id, p.topic, p.partition): p for p in previous_partitions
        }

        stuck: list[dict[str, object]] = []
        for current in current_partitions:
            key = (current.cluster_id, current.group_id, current.topic, current.partition)
            previous = previous_map.get(key)
            if previous is None:
                continue

            if current.ts - previous.ts < timedelta(seconds=thresholds.duration_s_ge):
                continue

            if not detector.is_stuck(current, previous):
                continue

            stuck_partition = detector.create_stuck_partition(current, previous, previous.ts)
            stuck.append(
                {
                    "topic": stuck_partition.topic,
                    "partition": stuck_partition.partition,
                    "assigned_member_id": stuck_partition.assigned_member_id,
                    "since_ts": stuck_partition.since_ts,
                    "current_lag": stuck_partition.current_lag,
                    "delta_committed": stuck_partition.delta_committed,
                    "delta_lag": stuck_partition.delta_lag,
                }
            )

        stuck.sort(key=lambda item: item["current_lag"], reverse=True)
        return stuck

    @staticmethod
    def _snapshot_to_partition(model: ConsumerPartitionSnapshotModel) -> ConsumerPartition:
        """ORM ìŠ¤ëƒ…ìƒ· ëª¨ë¸ â†’ Domain ConsumerPartition ë³€í™˜"""
        return ConsumerPartition(
            cluster_id=model.cluster_id,
            group_id=model.group_id,
            topic=model.topic,
            partition=model.partition,
            ts=model.ts,
            committed_offset=model.committed_offset,
            latest_offset=model.latest_offset,
            lag=model.lag,
            assigned_member_id=model.assigned_member_id,
        )


class GetGroupMembersUseCase:
    """Consumer Group ë©¤ë²„ ëª©ë¡ Use Case - ì‹¤ì‹œê°„ Kafka ì¡°íšŒ"""

    def __init__(self, admin_client_getter: Callable[[str], Awaitable[AdminClient]]) -> None:
        """Use case ìƒì„±ì

        Args:
            admin_client_getter: cluster_idë¡œ AdminClientë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
        """
        self._admin_client_getter = admin_client_getter

    async def execute(self, cluster_id: str, group_id: str) -> list[MemberDetailResponse]:
        """ë©¤ë²„ ëª©ë¡ ì¡°íšŒ - ì‹¤ì‹œê°„ Kafka ì¡°íšŒ

        Args:
            cluster_id: í´ëŸ¬ìŠ¤í„° ID
            group_id: Consumer Group ID

        Returns:
            ë©¤ë²„ ìƒì„¸ ëª©ë¡

        Raises:
            ValueError: AdminClientë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ê·¸ë£¹ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ
        """
        # 1. AdminClient ë° Adapter ìƒì„±
        admin_client = await self._admin_client_getter(cluster_id)

        adapter = KafkaConsumerAdapter(admin_client)
        collector = ConsumerDataCollector(adapter, cluster_id)

        # 2. ì‹¤ì‹œê°„ Kafka ì¡°íšŒ
        try:
            members = await collector.collect_members(group_id)
            partitions = await collector.collect_partitions(group_id)
        except KeyError:
            # ê·¸ë£¹ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            return []

        # 3. ë©¤ë²„ë³„ íŒŒí‹°ì…˜ ë§¤í•‘
        partitions_by_member: dict[str, list[dict[str, int | str]]] = {}
        for p in partitions:
            if p.assigned_member_id:
                partitions_by_member.setdefault(p.assigned_member_id, []).append(
                    {"topic": p.topic, "partition": p.partition}
                )

        # 4. ì‘ë‹µ ì¡°ë¦½
        return [
            MemberDetailResponse(
                member_id=m.member_id,
                client_id=m.client_id,
                client_host=m.client_host,
                assigned_partitions=partitions_by_member.get(m.member_id, []),
            )
            for m in members
        ]


class GetGroupPartitionsUseCase:
    """Consumer Group íŒŒí‹°ì…˜ ëª©ë¡ Use Case - ì‹¤ì‹œê°„ Kafka ì¡°íšŒ"""

    def __init__(self, admin_client_getter: Callable[[str], Awaitable[AdminClient]]) -> None:
        """Use case ìƒì„±ì

        Args:
            admin_client_getter: cluster_idë¡œ AdminClientë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
        """
        self._admin_client_getter = admin_client_getter

    async def execute(self, cluster_id: str, group_id: str) -> list[PartitionDetailResponse]:
        """íŒŒí‹°ì…˜ ëª©ë¡ ì¡°íšŒ - ì‹¤ì‹œê°„ Kafka ì¡°íšŒ

        Args:
            cluster_id: í´ëŸ¬ìŠ¤í„° ID
            group_id: Consumer Group ID

        Returns:
            íŒŒí‹°ì…˜ ìƒì„¸ ëª©ë¡

        Raises:
            ValueError: AdminClientë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ê·¸ë£¹ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ
        """
        # 1. AdminClient ë° Adapter ìƒì„±
        admin_client = await self._admin_client_getter(cluster_id)

        adapter = KafkaConsumerAdapter(admin_client)
        collector = ConsumerDataCollector(adapter, cluster_id)

        # 2. ì‹¤ì‹œê°„ Kafka ì¡°íšŒ
        try:
            partitions = await collector.collect_partitions(group_id)
        except KeyError:
            # ê·¸ë£¹ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            return []

        # 3. ì‘ë‹µ ì¡°ë¦½
        return [
            PartitionDetailResponse(
                topic=p.topic,
                partition=p.partition,
                committed_offset=p.committed_offset,
                latest_offset=p.latest_offset,
                lag=p.lag,
                assigned_member_id=p.assigned_member_id,
            )
            for p in partitions
        ]


class GetGroupRebalanceUseCase:
    """Consumer Group ë¦¬ë°¸ëŸ°ìŠ¤ ì´ë²¤íŠ¸ Use Case"""

    def __init__(
        self, session_factory: Callable[[], AbstractAsyncContextManager[AsyncSession]]
    ) -> None:
        self._session_factory = session_factory

    async def execute(
        self, cluster_id: str, group_id: str, limit: int = 10
    ) -> list[RebalanceEventResponse]:
        async with self._session_factory() as session:
            stmt = (
                select(ConsumerGroupRebalanceDeltaModel)
                .where(
                    ConsumerGroupRebalanceDeltaModel.cluster_id == cluster_id,
                    ConsumerGroupRebalanceDeltaModel.group_id == group_id,
                )
                .order_by(desc(ConsumerGroupRebalanceDeltaModel.ts))
                .limit(limit)
            )
            result = await session.execute(stmt)
            delta_models = list(result.scalars().all())

        return [
            RebalanceEventResponse(
                ts=d.ts,
                moved_partitions=d.moved_partitions,
                join_count=d.join_count,
                leave_count=d.leave_count,
                elapsed_since_prev_s=d.elapsed_since_prev_s,
                state=d.state,
            )
            for d in delta_models
        ]


class GetTopicConsumersUseCase:
    """í† í”½ë³„ ì»¨ìŠˆë¨¸ ë§¤í•‘ Use Case - ì‹¤ì‹œê°„ Kafka ì¡°íšŒ"""

    def __init__(self, admin_client_getter: Callable[[str], Awaitable[AdminClient]]) -> None:
        """Use case ìƒì„±ì

        Args:
            admin_client_getter: cluster_idë¡œ AdminClientë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
        """
        self._admin_client_getter = admin_client_getter

    async def execute(self, cluster_id: str, topic: str) -> TopicConsumerMappingResponse:
        """í† í”½ë³„ ì»¨ìŠˆë¨¸ ë§¤í•‘ ì¡°íšŒ - ì‹¤ì‹œê°„ Kafka ì¡°íšŒ

        Args:
            cluster_id: í´ëŸ¬ìŠ¤í„° ID
            topic: í† í”½ ì´ë¦„

        Returns:
            TopicConsumerMappingResponse

        Raises:
            ValueError: AdminClientë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ
        """
        # 1. AdminClient ë° Adapter ìƒì„±
        admin_client = await self._admin_client_getter(cluster_id)

        adapter = KafkaConsumerAdapter(admin_client)
        collector = ConsumerDataCollector(adapter, cluster_id)

        # 2. ëª¨ë“  Consumer Group ëª©ë¡ ì¡°íšŒ
        try:
            group_infos = await adapter.list_consumer_groups()
            all_groups = [
                await collector.collect_group(group_info.group_id) for group_info in group_infos
            ]

        except Exception as e:
            # ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë¡œê¹… í›„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            logging.error(f"Failed to collect consumer groups for topic {topic}: {e}")
            return TopicConsumerMappingResponse(topic=topic, consumer_groups=[])

        # 3. í•´ë‹¹ í† í”½ì„ êµ¬ë…í•˜ëŠ” ê·¸ë£¹ í•„í„°ë§
        consumer_groups: list[dict] = []

        logging.warning(
            f"ğŸ” [Topic Filter] Filtering topic '{topic}' from {len(all_groups)} consumer groups"
        )

        for group in all_groups:
            try:
                # ê° ê·¸ë£¹ì˜ íŒŒí‹°ì…˜ ì¡°íšŒ
                partitions = await collector.collect_partitions(group.group_id)
                logging.warning(
                    f"ğŸ“Š [Group Partitions] Group '{group.group_id}' has {len(partitions)} partitions: "
                    f"{[(p.topic, p.partition) for p in partitions]}"
                )

                # í•´ë‹¹ í† í”½ì˜ íŒŒí‹°ì…˜ë§Œ í•„í„°ë§
                topic_partitions = [p for p in partitions if p.topic == topic]

                if topic_partitions:
                    logging.warning(
                        f"âœ… [Match Found] Group '{group.group_id}' consumes topic '{topic}' "
                        f"({len(topic_partitions)} partitions)"
                    )

                    # Frontendê°€ í•„ìš”ë¡œ í•˜ëŠ” ëª¨ë“  í•„ë“œ í¬í•¨
                    consumer_groups.append(
                        {
                            "group_id": group.group_id,
                            "state": group.state.value,
                            "member_count": group.member_count,
                            # Frontendê°€ ê¸°ëŒ€í•˜ëŠ” ì¶”ê°€ í•„ë“œë“¤ (ê¸°ë³¸ê°’ ì„¤ì •)
                            "slo_compliance": getattr(group, "slo_compliance", 0.0),
                            "lag_p50": getattr(group, "lag_p50", 0),
                            "lag_p95": getattr(group, "lag_p95", 0),
                            "lag_max": max((p.lag for p in topic_partitions), default=0),
                            "stuck_count": getattr(group, "stuck_count", 0),
                            "rebalance_score": getattr(group, "rebalance_score", 0.0),
                            "fairness_gini": getattr(group, "fairness_gini", 0.0),
                            "recommendation": getattr(group, "recommendation", None),
                            "partitions": [
                                {
                                    "partition": p.partition,
                                    "assigned_member_id": p.assigned_member_id,
                                    "lag": p.lag,
                                }
                                for p in topic_partitions
                            ],
                        }
                    )
            except Exception as e:
                # ê°œë³„ ê·¸ë£¹ ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ë¡œê¹…í•˜ê³  ê±´ë„ˆëœ€
                logging.warning(f"Failed to collect partitions for group {group.group_id}: {e}")
                continue

        return TopicConsumerMappingResponse(
            topic=topic,
            consumer_groups=consumer_groups,
        )
